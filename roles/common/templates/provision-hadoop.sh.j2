#!/bin/bash

MASTER_IP={{ hostvars[groups['master'][0]][ansible_routing_network_interface]['ipv4']['address'] }}
NUM_SLAVES={{ groups['slaves'] | length }}
SLAVE_IPS={% set comma = joiner(",") %}
                              {%- for host in groups['slaves'] -%}
                                {{ comma() }}{{ hostvars[host][ansible_routing_network_interface]['ipv4']['address'] }}
                              {%- endfor %}

CE_GROUP={{ container_executor_group }}
CGROUP_HIERARCHY={{ cgroups_hierarchy }}
SCRIPT_LIST="set-hadoop-loglevel.sh test-dshell-docker-yarn.sh test-pi-docker-yarn.sh test-pi-docker-yarn-am.sh test-pi-yarn.sh test-dshell-yarn.sh restart-nodemanager.sh restart-yarn.sh restart-hdfs.sh start-nodemanager.sh add-docker-config.sh test-dshell-docker-yarn-mounts.sh test-pi-docker-yarn-mounts.sh kill-all-apps.sh build-docker-image-from-current-install.sh test-dshell-yarn-hdpuser.sh create-cgroup-dirs.sh test-sleep-yarn.sh test-sleep-docker-yarn.sh test-sleep-docker-yarn-am.sh restart-resourcemanager.sh"
{% raw %}
set -e

if [ -d "${HADOOP_HOME}" ]
then
  echo "WARNING: hadoop home : ${HADOOP_HOME} already exists. deleting."
  fuser -uk /home/yarn
  fuser -uk /home/hdfs
  rm -fr ${HADOOP_HOME}
fi

echo "creating new hadoop home: "
mkdir -p ${HADOOP_HOME}

echo "extracting hadoop archive ... "
tar -zxvf ${HADOOP_ARCHIVE} -C ${HADOOP_HOME} --strip-components=1

echo "creating hadoop environment script... "
cat <<EOF > ${ENV_CONFIG}
export JAVA_HOME=$JAVA_HOME
export HADOOP_HOME=${HADOOP_HOME}
export HADOOP_MAPRED_HOME=${HADOOP_HOME}
export HADOOP_COMMON_HOME=${HADOOP_HOME}
export HADOOP_HDFS_HOME=${HADOOP_HOME}
export YARN_HOME=${HADOOP_HOME}
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export YARN_CONF_DIR=${HADOOP_HOME}/etc/hadoop
EOF

echo "setting up core-site.xml .. "
cp ${HADOOP_INSTALL_DIR}/core-site.xml ${CORE_SITE}
sed -i -e "s;__MASTER_IP__;${MASTER_IP};g" -e "s;__HADOOP_HOME__;${HADOOP_HOME};g" ${CORE_SITE}

echo "setting up container-executor.cfg .. "
cp ${HADOOP_INSTALL_DIR}/container-executor.cfg ${CONTAINER_EXECUTOR_CFG}

echo "setting up yarn-site.xml .. "
cp ${HADOOP_INSTALL_DIR}/yarn-site.xml ${YARN_SITE}
sed -i "s;__MASTER_IP__;${MASTER_IP};g" ${YARN_SITE}

echo "setting up mapred-site.xml .. "
cp ${HADOOP_INSTALL_DIR}/mapred-site.xml ${MAPRED_SITE}

echo "setting up hadoop-env.sh .. "
cp ${HADOOP_INSTALL_DIR}/hadoop-env.sh ${HADOOP_ENV_SH}

echo "setting up capacity-scheduler.xml .. "
cp ${HADOOP_INSTALL_DIR}/capacity-scheduler.xml ${CAPACITY_SCHEDULER_CONFIG}

echo "setting up slaves file .. "
echo "slaves list : ${SLAVE_IPS}"
echo ${SLAVE_IPS} | sed "s/,/\n/g" > ${SLAVES}

echo "fixing permissions ... "
chown -R root:root ${HADOOP_HOME}
chown root:yarn ${HADOOP_HOME}/bin/container-executor 
chmod 6050 ${HADOOP_HOME}/bin/container-executor 
mkdir -p ${HADOOP_HOME}/logs
chmod 777 ${HADOOP_HOME}/logs
mkdir -p ${HADOOP_HOME}/tmp
chmod 777 ${HADOOP_HOME}/tmp

echo "done installing hadoop environment"

echo "copying test scripts"
for script in $SCRIPT_LIST; do
  echo "copying $SCRIPT"
  cp $script ${HADOOP_HOME}
done

echo "creating cgroup hierarchy"
for cgroup in $(mount | grep ^cgroup | awk '{print $3}'); do
  mkdir -p ${cgroup}/${CGROUP_HIERARCHY}
  chown -R ${CE_GROUP}:${CE_GROUP} ${cgroup}/${CGROUP_HIERARCHY}
done

echo "Create directories for bind mount tests"
mkdir -p /tmp/readwrite
chown nobody:nobody /tmp/readwrite
mkdir -p /tmp/readonly
chown nobody:nobody /tmp/readonly

{% endraw %}
